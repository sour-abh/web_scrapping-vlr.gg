{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PYTHON we scrapping project"
      ],
      "metadata": {
        "id": "ySJRQjqmTSjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#scrapping web data using Python\n"
      ],
      "metadata": {
        "id": "bw9fxtxh_AUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting data from valorant(game) forum website vlr.gg\n",
        "\n",
        "1.   scrapping the thread data from vlr.gg(website)\n",
        "2.   tools\n",
        "\n",
        "    *   python\n",
        "    *   requests: to download the webpage\n",
        "    *   BeautifulSoup: to parse the HTML \n",
        "    *   Pandas: to handle the data and convert it into different(desired) format\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bCZcSTrVNcko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here are the steps to follow\n",
        " * we are gonna scrape the url https://vlr.gg/threads\n",
        " * we are gonna extract the different thread title,author,author url,post comment,post url and post text\n",
        " *then we will save all the data in csv "
      ],
      "metadata": {
        "id": "1YV4WMlcPI_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use the request library to download webpages"
      ],
      "metadata": {
        "id": "XsVyE_4r-_g2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#importing all the Libraries"
      ],
      "metadata": {
        "id": "tjSHH1PZ2N7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing all the libraries\n"
      ],
      "metadata": {
        "id": "F8yeCiw9Nrcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import all the libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re "
      ],
      "metadata": {
        "id": "SR5hKgFilsNG"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a thread Extract function"
      ],
      "metadata": {
        "id": "UBjNG48qQEoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in thread extract function \n",
        "  * we have defined an empty dictionary with the name of all the attributes we want to extract\n",
        "  * using requests downloading the webpage\n",
        "  *then using beautifulsoup to parse the html\n",
        "  * using beautifulsoup we have defined , the title tag,date,comment,and base url\n",
        "  * using a loop to save all the data in dictionary dict2"
      ],
      "metadata": {
        "id": "5TYaai99QQz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting title ,date,comments ,post url from the thread page of Vlr.gg \n",
        "\n",
        "def thread_extract(topic_url):\n",
        "    dict2={'title':[],'date_posted':[],'comments':[],'post_url':[],'author':[],'author_url':[],'post_text':[]}\n",
        "    #initiating the response checking for status code\n",
        "    response=requests.get(topic_url)\n",
        "    #if status code not equals to 200 return\n",
        "    if response.status_code !=200:\n",
        "        return\n",
        "    #parsing html using beautiful soup\n",
        "    doc=BeautifulSoup(response.text,'html.parser')\n",
        "    #title anchor tag \n",
        "    title_a_tag=doc.find_all('a',{'class':'thread-item-header-title'})\n",
        "    #parsing date posted\n",
        "    date_posted=doc.find_all('span',{'class':\"date-full hide\"})\n",
        "    #parsing the total comments\n",
        "    comment=doc.find_all('span',{'class':'post-count'})\n",
        "    #extracting the post_url\n",
        "    post_url=doc.find_all('a',{'class':\"thread-item-header-title\"})\n",
        "    base_url='https://vlr.gg'\n",
        "    i=0;\n",
        "    while i<len(title_a_tag):\n",
        "        dict2['title'].append(title_a_tag[i].text.strip())\n",
        "\n",
        "        dict2['comments'].append(re.findall('\\d+',comment[i].text)) #using regular expression to extract the number from comments\n",
        "        \n",
        "        dict2['date_posted'].append(date_posted[i].text.strip())\n",
        "\n",
        "        dict2['post_url'].append(base_url+post_url[i]['href'])\n",
        "        i+=1\n",
        "    return dict2\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "isO2bALqf_lz"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a helper function to extract autor name ,author url and post detail"
      ],
      "metadata": {
        "id": "2BpvDp0FRTHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using the different post url contained in dictionary we will download the webpages of different url and parse them using beautifulSoup\n",
        "then we will save the data in dictionary using while loops \n",
        "\n",
        "**note the code is not well optimised**"
      ],
      "metadata": {
        "id": "bfO4wTBkRebT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the helper function to extract the data \n",
        "def get_post_details(response):\n",
        "    #defining the variables\n",
        "    i=j=k=0\n",
        "    #initialising the Beautiful soup for all \n",
        "    post_doc=BeautifulSoup(response.text,'html.parser')\n",
        "    author=post_doc('a',{'class':'post-header-author'})\n",
        "    base_url='https://vlr.gg'\n",
        "    div_post_body=post_doc.find_all('div',{'class':\"post-body\"})\n",
        "    author_name=author[0].text.strip()\n",
        "    author_url=base_url+author[0]['href']\n",
        "    post_text=[]\n",
        "    while i < len(div_post_body):\n",
        "        while j < len(div_post_body[i]('p')):\n",
        "            post_text.append(div_post_body[i]('p')[j].text.strip())\n",
        "            j+=1\n",
        "        while k < len(div_post_body[i]('a')):\n",
        "            post_text.append(div_post_body[i]('a')[k]['href'])\n",
        "            k+=1\n",
        "        i+=1\n",
        "    return [author_name,author_url,post_text]"
      ],
      "metadata": {
        "id": "mdYPwwPl2vKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#using helper function in our main Post_extract function"
      ],
      "metadata": {
        "id": "GGuGe_c7SgL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using a loop saving the data in dictionary"
      ],
      "metadata": {
        "id": "ZP0eKusZSr1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_extract(dict2):\n",
        "    #    dict2={'title':[],'date_posted':[],'comments':[],'post_url':[],'author':[],'author_url':[],'post_text':[]}\n",
        "    #getting the webpage\n",
        "    x=0\n",
        "    while x < len(dict2['post_url']):\n",
        "    # downloading all the page url in loop\n",
        "        response=requests.get(dict2['post_url'][x])\n",
        "    #parsing using BeautifulSoup\n",
        "        dict2[\"author\"].append(get_post_details(response)[0])\n",
        "        dict2[\"author_url\"].append(get_post_details(response)[1])\n",
        "        dict2[\"post_text\"].append(get_post_details(response)[2])\n",
        "        x+=1\n",
        "    # returning th dictionary containing author,author_url,and post_text\n",
        "    return dict2\n"
      ],
      "metadata": {
        "id": "KGhT8iXAjLWR"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving to CSV"
      ],
      "metadata": {
        "id": "9EIhFotWSzhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving it as csv\n",
        "def to_csv(dict2):\n",
        "    count=0\n",
        "    df=pd.DataFrame(dict2)\n",
        "    df.to_csv('page{}.csv'.format(count),index=None)"
      ],
      "metadata": {
        "id": "3HgH70Rp1ApT"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#using the functions"
      ],
      "metadata": {
        "id": "yQWb24VdS8ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_url=input('place the link: ')\n",
        "a=thread_extract(topic_url)\n",
        "data=post_extract(a)\n",
        "to_csv(data)\n"
      ],
      "metadata": {
        "id": "xCdIlau7kHhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560f912c-5880-42fa-d59f-7ff8ad564c5a"
      },
      "execution_count": 153,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "place the link: https://vlr.gg/threads\n"
          ]
        }
      ]
    }
  ]
}